{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import sklearn\n",
    "from skimage import io\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "from load_animals import *\n",
    "from influence.iter_attack import get_projection_to_box_around_orig_point\n",
    "from influence.inceptionModel import BinaryInceptionModel\n",
    "from data_poisoning import data_poisoning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from utils import dataset_metadatas, experiment_result_metadata_to_FN, FN_to_experiment_result_metadata, get_dataset, render_img, get_full_model_graph\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_name_exp2(fn):\n",
    "    fn = fn[:-4]\n",
    "    lst = fn.split('_')\n",
    "    content_type = lst[2]\n",
    "    approach = lst[3]\n",
    "    target_label = lst[7]\n",
    "    test_idx = lst[-1]\n",
    "    return {\n",
    "        \"content_type\": content_type,\n",
    "        \"approach\": approach,\n",
    "        \"target_label\": target_label,\n",
    "        \"test_idx\": test_idx\n",
    "    }\n",
    "\n",
    "def sorted_dir_exp2(folder, order_by = False):\n",
    "    def getmtime(name):\n",
    "        path = os.path.join(folder, name)\n",
    "        return os.path.getmtime(path)\n",
    "    def order(name):\n",
    "        if name == '.ipynb_checkpoints' or name == 'Analysis': return (1000, 1000)\n",
    "        file = parse_file_name_exp2(name)\n",
    "        return (int(file[\"test_idx\"]), int(file[\"target_label\"]) + int(file[\"test_idx\"]))\n",
    "    \n",
    "    if order_by:\n",
    "        return sorted(os.listdir(folder), key=order)\n",
    "\n",
    "    return sorted(os.listdir(folder), key=getmtime, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading animals from disk...\n",
      "../data/dataset_Eagle-Mushroom-Snail_train-500_test-50.npz\n"
     ]
    }
   ],
   "source": [
    "dataset_classes = dataset_metadatas[\"Eagle-Mushroom-Snail\"]\n",
    "data_sets = get_dataset(dataset_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.logits Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[genericNeuralNet.py:191 -   get_vec_to_list_fn() ] Total number of parameters: 6144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong_labels_bool Tensor(\"Shape_2:0\", shape=(2,), dtype=int32)\n",
      "logits Tensor(\"Shape_3:0\", shape=(2,), dtype=int32)\n",
      "inception_features:  Tensor(\"flatten/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "x_poison_features:  Tensor(\"StridedSlice:0\", dtype=float32)\n",
      "t_target_features:  Tensor(\"Gather:0\", shape=(1, ?), dtype=float32)\n",
      "Lp:  Tensor(\"Mean_1:0\", dtype=float32)\n",
      "LP_gradient Tensor(\"strided_slice_1:0\", shape=(?, 268203), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "full_graph, full_model = get_full_model_graph(dataset_classes, data_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1.0 / (len(data_sets.train.x) * .001) \n",
    "top_model = LogisticRegression(\n",
    "                        C=C,\n",
    "                        tol=1e-8,\n",
    "                        fit_intercept=False, \n",
    "                        solver='lbfgs',\n",
    "                        multi_class='multinomial',\n",
    "                        warm_start=True, #True\n",
    "                        max_iter=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "with full_graph.as_default():\n",
    "    clean_inception_features = full_model.generate_inception_features(data_sets.train, None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp2_analysis(approach):\n",
    "\n",
    "    source_dir = \"Experiment_results/Experiment_2_redo/\" \n",
    "    file_names = list((sorted_dir_exp2(source_dir, True)))\n",
    "    if 'Analysis' in file_names:\n",
    "        file_names.remove('Analysis')\n",
    "    if '.ipynb_checkpoints' in file_names:\n",
    "        file_names.remove('.ipynb_checkpoints')\n",
    "\n",
    "    poisoned_train_indices, poisoned_images, target_test_indices, target_labels = [], [], [], []\n",
    "    for i in range(0, len(file_names), 2):\n",
    "        file_name = file_names[i]\n",
    "        # only look at the results from the specified dataset\n",
    "        file = parse_file_name_exp2(file_name)\n",
    "        if file[\"approach\"] == approach:\n",
    "            if file[\"content_type\"] == 'indices':\n",
    "                poisoned_train_index = np.load(source_dir + file_name)\n",
    "                poisoned_image = np.load(source_dir + file_names[i+1])\n",
    "            else:\n",
    "                poisoned_train_index = np.load(source_dir + file_names[i+1])\n",
    "                poisoned_image = np.load(source_dir + file_name)\n",
    "            target_test_index = file['test_idx']\n",
    "            poisoned_images.append(poisoned_image)\n",
    "            poisoned_train_indices.append(poisoned_train_index)\n",
    "            target_test_indices.append(target_test_index)\n",
    "            target_labels.append(file['target_label'])\n",
    "\n",
    "    polluted_inception_features = []\n",
    "    with full_graph.as_default():\n",
    "        for poison_image, poisoned_train_index in zip(poisoned_images, poisoned_train_indices):\n",
    "            poisoned_dataset = DataSet(poison_image, data_sets.train.labels[poisoned_train_index.astype(int)])\n",
    "            polluted_inception_feature = full_model.generate_inception_features(poisoned_dataset, None)\n",
    "            polluted_inception_features.append(polluted_inception_feature)\n",
    "\n",
    "    path_to_save = \"Experiment_results/Experiment_2_redo/Analysis/\" + approach + '/'\n",
    "\n",
    "    for counter, (polluted_inception_feature, poisoned_train_index, target_test_index, target_label) in enumerate(zip(polluted_inception_features, poisoned_train_indices, target_test_indices, target_labels)):\n",
    "        target_test_dataset = DataSet(np.copy(data_sets.test.x[[int(target_test_index)]]), np.copy(data_sets.test.labels[[int(target_test_index)]]))\n",
    "        with full_graph.as_default():\n",
    "            target_test_inception_feature = full_model.generate_inception_features(target_test_dataset, None)   \n",
    "        X, Y = np.copy(clean_inception_features), np.copy(data_sets.train.labels)\n",
    "        X[poisoned_train_index.astype(int)] = polluted_inception_feature\n",
    "        top_model.fit(X, Y)    \n",
    "        logits_true_label = top_model.predict_proba(target_test_inception_feature)[0][int(data_sets.test.labels[int(target_test_index)])]\n",
    "        experiment_result_metadata = {\n",
    "        \"Experiment_number\":2,\n",
    "        \"contents_type\":\"logits\",\n",
    "        \"method\":approach,\n",
    "        \"dataset_name\":dataset_classes[\"name\"],\n",
    "        \"num_poisoned_training_points\":int(target_label),\n",
    "        \"test_idx\":int(data_sets.test.labels[int(target_test_index)])\n",
    "        }\n",
    "        FN = experiment_result_metadata_to_FN(experiment_result_metadata)\n",
    "        np.save(path_to_save+experiment_result_metadata_to_FN(experiment_result_metadata)+\"_\" +str(counter), logits_true_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True stands for M <-> S\n",
    "# False stands for E <-> M/S\n",
    "target_type = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_name_exp2_plotting(fn):\n",
    "    fn = fn[:-4]\n",
    "    lst = fn.split('_')\n",
    "    approach = lst[3]\n",
    "    test_label = lst[-2]\n",
    "    train_label = lst[7]\n",
    "    return {\n",
    "        \"train_label\": train_label,\n",
    "        \"approach\": approach,\n",
    "        \"test_label\": test_label,\n",
    "    }\n",
    "\n",
    "def sorted_dir_exp2_plotting(folder):\n",
    "    def getmtime(name):\n",
    "        path = os.path.join(folder, name)\n",
    "        return os.path.getmtime(path)\n",
    "    return sorted(os.listdir(folder), key=getmtime, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits_given_approach(approach):\n",
    "    source_dir = \"Experiment_results/Experiment_2_redo/Analysis/\" + approach + '/' \n",
    "    file_names = list((sorted_dir_exp2_plotting(source_dir)))\n",
    "    if 'Analysis' in file_names:\n",
    "        file_names.remove('Analysis')\n",
    "    if '.ipynb_checkpoints' in file_names:\n",
    "        file_names.remove('.ipynb_checkpoints')\n",
    "\n",
    "    train_labels, test_labels, logits = [], [], []\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file = parse_file_name_exp2_plotting(file_name)\n",
    "        train_labels.append(file['train_label'])\n",
    "        test_labels.append(file['test_label'])\n",
    "        logits.append(np.load(source_dir+file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14,), (14,), (14,))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sets.test.labels[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:influence_env]",
   "language": "python",
   "name": "conda-env-influence_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
