{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/influence_env/lib/python2.7/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/influence_env/lib/python2.7/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/influence_env/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import IPython\n",
    "import copy\n",
    "from shutil import copyfile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.dont_write_bytecode=True\n",
    "\n",
    "PACKAGE_PARENT = '../'\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser('__file__'))))\n",
    "sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
    "\n",
    "from influence.inceptionModel import BinaryInceptionModel\n",
    "from influence.logisticRegressionWithLBFGS import LogisticRegressionWithLBFGS\n",
    "import influence.experiments\n",
    "from influence.dataset import DataSet\n",
    "# from influence.dataset_poisoning import iterative_attack, select_examples_to_attack, get_projection_to_box_around_orig_point, generate_inception_features\n",
    "from influence.iter_attack import iterative_attack, select_examples_to_attack, get_projection_to_box_around_orig_point, generate_inception_features\n",
    "from influence.Progress import *\n",
    "\n",
    "from utils import dataset_metadatas, experiment_result_metadata_to_FN, FN_to_experiment_result_metadata, get_dataset, get_full_model_graph\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from load_animals import *\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data_poisoning import data_poisoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal of this function is to research on the full model transferbility.\n",
    "# The top models we consider are Logistic Regression and SVM.\n",
    "\n",
    "# Given which dataset to use, num_to_perterb, IF or FC,\n",
    "# the function finds the results of data poisoning from the ./Experiment_results/Experiment_1/ folder\n",
    "# and give the transferbility result: the logits of the true label of target test point \n",
    "\n",
    "dataset_classes = dataset_metadatas[\"Dog-Fish\"] \n",
    "#num_train_ex_per_class, num_test_ex_per_class = 900, 300\n",
    "use_IF = True\n",
    "num_to_perterb = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading animals from disk...\n",
      "../data/dataset_dog-fish_train-900_test-300.npz\n"
     ]
    }
   ],
   "source": [
    "# 1. Get the clean inception data_set \n",
    "data_sets = get_dataset(dataset_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.logits Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[genericNeuralNet.py:191 -   get_vec_to_list_fn() ] Total number of parameters: 2048\n",
      "/home/ubuntu/anaconda3/envs/influence_env/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong_labels_bool Tensor(\"Shape_2:0\", shape=(2,), dtype=int32)\n",
      "logits Tensor(\"Shape_3:0\", shape=(2,), dtype=int32)\n",
      "inception_features:  Tensor(\"flatten/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "x_poison_features:  Tensor(\"Gather:0\", shape=(1, ?), dtype=float32)\n",
      "t_target_features:  Tensor(\"Gather_1:0\", shape=(1, ?), dtype=float32)\n",
      "Lp:  Tensor(\"norm/Squeeze:0\", shape=(), dtype=float32)\n",
      "LP_gradient Tensor(\"strided_slice_1:0\", shape=(268203,), dtype=float32)\n",
      "self.logits Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[genericNeuralNet.py:191 -   get_vec_to_list_fn() ] Total number of parameters: 1536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong_labels_bool Tensor(\"Shape_2:0\", shape=(2,), dtype=int32)\n",
      "logits Tensor(\"Shape_3:0\", shape=(2,), dtype=int32)\n",
      "inception_features:  Tensor(\"flatten/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "x_poison_features:  Tensor(\"Gather:0\", shape=(1, ?), dtype=float32)\n",
      "t_target_features:  Tensor(\"Gather_1:0\", shape=(1, ?), dtype=float32)\n",
      "Lp:  Tensor(\"norm/Squeeze:0\", shape=(), dtype=float32)\n",
      "LP_gradient Tensor(\"strided_slice_1:0\", shape=(268203,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "img_side = 299\n",
    "num_channels = 3 \n",
    "batch_size = 100\n",
    "initial_learning_rate = 0.001 \n",
    "keep_probs = None\n",
    "decay_epochs = [1000, 10000]\n",
    "weight_decay = 0.001\n",
    "    \n",
    "training_dataset_classes = dataset_classes[\"classes\"]\n",
    "num_classes = len(training_dataset_classes)\n",
    "full_graph_inception = tf.Graph()\n",
    "with full_graph_inception.as_default():\n",
    "    full_model_name = '%s_inception_wd-%s' % ('_'.join(training_dataset_classes), weight_decay)\n",
    "    full_model_inception = BinaryInceptionModel(\n",
    "        img_side=img_side,\n",
    "        num_channels=num_channels,\n",
    "        weight_decay=weight_decay,\n",
    "        num_classes=num_classes, \n",
    "        batch_size=batch_size,\n",
    "        data_sets=data_sets,\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        keep_probs=keep_probs,\n",
    "        decay_epochs=decay_epochs,\n",
    "        mini_batch=True,\n",
    "        train_dir='output',\n",
    "        log_dir='log',\n",
    "        model_name=full_model_name)\n",
    "    \n",
    "full_graph_inceptionResNet = tf.Graph()\n",
    "with full_graph_inceptionResNet.as_default():\n",
    "    full_model_name = '%s_inceptionResNet_wd-%s' % ('_'.join(training_dataset_classes), weight_decay)\n",
    "    full_model_inceptionResNet = BinaryInceptionModel(\n",
    "        img_side=img_side,\n",
    "        num_channels=num_channels,\n",
    "        weight_decay=weight_decay,\n",
    "        use_InceptionResNet = True,\n",
    "        num_classes=num_classes, \n",
    "        batch_size=batch_size,\n",
    "        data_sets=data_sets,\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        keep_probs=keep_probs,\n",
    "        decay_epochs=decay_epochs,\n",
    "        mini_batch=True,\n",
    "        train_dir='output',\n",
    "        log_dir='log',\n",
    "        model_name=full_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with full_graph_inception.as_default():\n",
    "    clean_inception_features = full_model_inception.generate_inception_features(data_sets.train, None)\n",
    "with full_graph_inceptionResNet.as_default():\n",
    "    clean_inceptionResNet_features = full_model_inceptionResNet.generate_inception_features(data_sets.train, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Find the results from the ./Experiment_results/Experiment_1/ folder\n",
    "def parse_file_name(fn):\n",
    "    fn = fn[:-4]\n",
    "    lst = fn.split('_')\n",
    "    test_idx = int(lst[-1])\n",
    "    num_poisoned_training_points = int(lst[-3])\n",
    "    Experiment_number = int(lst[1])\n",
    "    contents_type = lst[2]\n",
    "    method = lst[3]\n",
    "    return {\n",
    "        \"test_idx\":test_idx,\n",
    "        \"num_poisoned_training_points\":num_poisoned_training_points,\n",
    "        \"Experiment_number\":Experiment_number,\n",
    "        \"contents_type\":contents_type,\n",
    "        \"method\":method,\n",
    "    }\n",
    "\n",
    "source_dir = \"Experiment_results/Experiment_1/\"\n",
    "file_names = !ls -tr Experiment_results/Experiment_1/\n",
    "poisoned_images, poisoned_train_indices, target_test_indices = [], [], []\n",
    "for i in range(0, len(file_names), 2):\n",
    "    file_name = file_names[i]\n",
    "    dataset_name = dataset_classes['name'].replace('-', '_')\n",
    "    # only look at the results from the specified dataset\n",
    "    if dataset_name in file_name:\n",
    "        file = parse_file_name(file_name)\n",
    "        if file[\"num_poisoned_training_points\"] == num_to_perterb and file[\"Experiment_number\"] == 1 and ((use_IF and file[\"method\"] == 'IF') or (not use_IF and file[\"method\"] == 'FC')):\n",
    "            if file[\"contents_type\"] == 'indices':\n",
    "                poisoned_train_index = np.load(source_dir + file_name)\n",
    "                poisoned_image = np.load(source_dir + file_names[i+1])\n",
    "            else:\n",
    "                poisoned_train_index = np.load(source_dir + file_names[i+1])\n",
    "                poisoned_image = np.load(source_dir + file_name)\n",
    "            target_test_index = file['test_idx']\n",
    "            poisoned_images.append(poisoned_image)\n",
    "            poisoned_train_indices.append(poisoned_train_index)\n",
    "            target_test_indices.append(target_test_index)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Generate polluted inception features\n",
    "\n",
    "polluted_inception_features = []\n",
    "with full_graph_inception.as_default():\n",
    "    for poison_image, poisoned_train_index in zip(poisoned_images, poisoned_train_indices):\n",
    "        poisoned_dataset = DataSet(poison_image, data_sets.train.labels[poisoned_train_index])\n",
    "        polluted_inception_feature = full_model_inception.generate_inception_features(poisoned_dataset, None)\n",
    "        polluted_inception_features.append(polluted_inception_feature)\n",
    "       \n",
    "polluted_inceptionResNet_features = []\n",
    "with full_graph_inceptionResNet.as_default():\n",
    "    for poison_image, poisoned_train_index in zip(poisoned_images, poisoned_train_indices):\n",
    "        poisoned_dataset = DataSet(poison_image, data_sets.train.labels[poisoned_train_index])\n",
    "        polluted_inceptionResNet_feature = full_model_inceptionResNet.generate_inception_features(poisoned_dataset, None)\n",
    "        polluted_inceptionResNet_features.append(polluted_inceptionResNet_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Initialize the top models (Log Reg) firstly then compute the final results\n",
    "\n",
    "C = 1.0 / (len(data_sets.train.x) * .001)  \n",
    "if num_classes == 2:\n",
    "    log_reg_model_inception = LogisticRegression(\n",
    "                C=C,\n",
    "                tol=1e-8,\n",
    "                fit_intercept=False, \n",
    "                solver='lbfgs',\n",
    "                warm_start=True, #True\n",
    "                max_iter=1000)\n",
    "    log_reg_model_inceptionResNet = LogisticRegression(\n",
    "                C=C,\n",
    "                tol=1e-8,\n",
    "                fit_intercept=False, \n",
    "                solver='lbfgs',\n",
    "                warm_start=True, #True\n",
    "                max_iter=1000)\n",
    "else:\n",
    "    log_reg_model_inception = LogisticRegression(\n",
    "                C=C,\n",
    "                tol=1e-8,\n",
    "                fit_intercept=False, \n",
    "                solver='lbfgs',\n",
    "                multi_class='multinomial',\n",
    "                warm_start=True, #True\n",
    "                max_iter=1000) \n",
    "    log_reg_model_inceptionResNet = LogisticRegression(\n",
    "                C=C,\n",
    "                tol=1e-8,\n",
    "                fit_intercept=False, \n",
    "                solver='lbfgs',\n",
    "                multi_class='multinomial',\n",
    "                warm_start=True, #True\n",
    "                max_iter=1000)\n",
    "\n",
    "logits_inception, logits_inceptionResNet = [], []\n",
    "for polluted_inception_feature, polluted_inceptionResNet_feature, poisoned_train_index, target_test_index in zip(polluted_inception_features, polluted_inceptionResNet_features, poisoned_train_indices, target_test_indices):\n",
    "    target_test_dataset = DataSet(np.copy(data_sets.test.x[[target_test_index]]), np.copy(data_sets.test.labels[[target_test_index]]))\n",
    "    with full_graph_inception.as_default():\n",
    "        target_test_inception_feature = full_model_inception.generate_inception_features(target_test_dataset, None)   \n",
    "    X, Y = np.copy(clean_inception_features), np.copy(data_sets.train.labels)\n",
    "    X[poisoned_train_index] = polluted_inception_feature\n",
    "    log_reg_model_inception.fit(X, Y)    \n",
    "    logits_inception.append(log_reg_model_inception.predict_proba(target_test_inception_feature)[0][int(data_sets.test.labels[target_test_index])])\n",
    "    \n",
    "    with full_graph_inceptionResNet.as_default():\n",
    "        target_test_inceptionResNet_feature = full_model_inceptionResNet.generate_inception_features(target_test_dataset, None)   \n",
    "    X, Y = np.copy(clean_inceptionResNet_features), np.copy(data_sets.train.labels)\n",
    "    X[poisoned_train_index] = polluted_inceptionResNet_feature\n",
    "    log_reg_model_inceptionResNet.fit(X, Y)    \n",
    "    logits_inceptionResNet.append(log_reg_model_inceptionResNet.predict_proba(target_test_inceptionResNet_feature)[0][int(data_sets.test.labels[target_test_index])])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.026601219866971445,\n",
       " 0.9835105062431875,\n",
       " 0.025527452686357033,\n",
       " 0.00986500607154539,\n",
       " 0.010891309051715203,\n",
       " 0.005563066879134992,\n",
       " 0.7506454908263247]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9976384792791324,\n",
       " 0.9997061453241939,\n",
       " 0.9996707425811158,\n",
       " 0.9996608672139135,\n",
       " 0.9999209242376349,\n",
       " 0.9979180232567562,\n",
       " 0.9997762087366919]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_inceptionResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:influence_env]",
   "language": "python",
   "name": "conda-env-influence_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
