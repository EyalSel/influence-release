{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import IPython\n",
    "import copy\n",
    "from shutil import copyfile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.dont_write_bytecode=True\n",
    "\n",
    "PACKAGE_PARENT = '../'\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser('__file__'))))\n",
    "sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
    "\n",
    "from influence.inceptionModel import BinaryInceptionModel\n",
    "from influence.logisticRegressionWithLBFGS import LogisticRegressionWithLBFGS\n",
    "import influence.experiments\n",
    "from influence.dataset import DataSet\n",
    "# from influence.dataset_poisoning import iterative_attack, select_examples_to_attack, get_projection_to_box_around_orig_point, generate_inception_features\n",
    "from influence.iter_attack import iterative_attack, select_examples_to_attack, get_projection_to_box_around_orig_point, generate_inception_features\n",
    "from influence.Progress import *\n",
    "\n",
    "from utils import dataset_metadatas, experiment_result_metadata_to_FN, FN_to_experiment_result_metadata, get_dataset, get_full_model_graph\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from load_animals import *\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data_poisoning import data_poisoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal of this function is to research on the top model transferbility.\n",
    "# The top models we consider are Logistic Regression and SVM.\n",
    "\n",
    "# Given which dataset to use, num_to_perterb, IF or FC,\n",
    "# the function finds the results of data poisoning from the ./Experiment_results/Experiment_1/ folder\n",
    "# and give the transferbility result: the logits of the true label of target test point \n",
    "\n",
    "dataset_classes = dataset_metadatas[\"Dog-Fish\"] \n",
    "#num_train_ex_per_class, num_test_ex_per_class = 900, 300\n",
    "use_IF = True\n",
    "num_to_perterb = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading animals from disk...\n",
      "../data/dataset_dog-fish_train-900_test-300.npz\n"
     ]
    }
   ],
   "source": [
    "# 1. Get the clean inception data_set \n",
    "data_sets = get_dataset(dataset_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.logits Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[genericNeuralNet.py:191 -   get_vec_to_list_fn() ] Total number of parameters: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong_labels_bool Tensor(\"Shape_2:0\", shape=(2,), dtype=int32)\n",
      "logits Tensor(\"Shape_3:0\", shape=(2,), dtype=int32)\n",
      "inception_features:  Tensor(\"flatten/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "x_poison_features:  Tensor(\"Gather:0\", shape=(1, ?), dtype=float32)\n",
      "t_target_features:  Tensor(\"Gather_1:0\", shape=(1, ?), dtype=float32)\n",
      "Lp:  Tensor(\"norm/Squeeze:0\", shape=(), dtype=float32)\n",
      "LP_gradient Tensor(\"strided_slice_1:0\", shape=(268203,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "img_side = 299\n",
    "num_channels = 3 \n",
    "batch_size = 100\n",
    "initial_learning_rate = 0.001 \n",
    "keep_probs = None\n",
    "decay_epochs = [1000, 10000]\n",
    "weight_decay = 0.001\n",
    "    \n",
    "training_dataset_classes = dataset_classes[\"classes\"]\n",
    "num_classes = len(training_dataset_classes)\n",
    "full_graph = tf.Graph()\n",
    "with full_graph.as_default():\n",
    "    full_model_name = '%s_inception_wd-%s' % ('_'.join(training_dataset_classes), weight_decay)\n",
    "    full_model = BinaryInceptionModel(\n",
    "        img_side=img_side,\n",
    "        num_channels=num_channels,\n",
    "        weight_decay=weight_decay,\n",
    "        num_classes=num_classes, \n",
    "        batch_size=batch_size,\n",
    "        data_sets=data_sets,\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        keep_probs=keep_probs,\n",
    "        decay_epochs=decay_epochs,\n",
    "        mini_batch=True,\n",
    "        train_dir='output',\n",
    "        log_dir='log',\n",
    "        model_name=full_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with full_graph.as_default():\n",
    "    clean_inception_features = full_model.generate_inception_features(data_sets.train, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Find the results from the ./Experiment_results/Experiment_1/ folder\n",
    "def parse_file_name(fn):\n",
    "    fn = fn[:-4]\n",
    "    lst = fn.split('_')\n",
    "    test_idx = int(lst[-1])\n",
    "    num_poisoned_training_points = int(lst[-3])\n",
    "    Experiment_number = int(lst[1])\n",
    "    contents_type = lst[2]\n",
    "    method = lst[3]\n",
    "    return {\n",
    "        \"test_idx\":test_idx,\n",
    "        \"num_poisoned_training_points\":num_poisoned_training_points,\n",
    "        \"Experiment_number\":Experiment_number,\n",
    "        \"contents_type\":contents_type,\n",
    "        \"method\":method,\n",
    "    }\n",
    "\n",
    "source_dir = \"Experiment_results/Experiment_1/\"\n",
    "file_names = !ls -tr Experiment_results/Experiment_1/\n",
    "poisoned_images, poisoned_train_indices, target_test_indices = [], [], []\n",
    "for i in range(0, len(file_names), 2):\n",
    "    file_name = file_names[i]\n",
    "    dataset_name = dataset_classes['name'].replace('-', '_')\n",
    "    # only look at the results from the specified dataset\n",
    "    if dataset_name in file_name:\n",
    "        file = parse_file_name(file_name)\n",
    "        if file[\"num_poisoned_training_points\"] == num_to_perterb and file[\"Experiment_number\"] == 1 and ((use_IF and file[\"method\"] == 'IF') or (not use_IF and file[\"method\"] == 'FC')):\n",
    "            if file[\"contents_type\"] == 'indices':\n",
    "                poisoned_train_index = np.load(source_dir + file_name)\n",
    "                poisoned_image = np.load(source_dir + file_names[i+1])\n",
    "            else:\n",
    "                poisoned_train_index = np.load(source_dir + file_names[i+1])\n",
    "                poisoned_image = np.load(source_dir + file_name)\n",
    "            target_test_index = file['test_idx']\n",
    "            poisoned_images.append(poisoned_image)\n",
    "            poisoned_train_indices.append(poisoned_train_index)\n",
    "            target_test_indices.append(target_test_index)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Generate polluted inception features\n",
    "\n",
    "polluted_inception_features = []\n",
    "with full_graph.as_default():\n",
    "    for poison_image, poisoned_train_index in zip(poisoned_images, poisoned_train_indices):\n",
    "        poisoned_dataset = DataSet(poison_image, data_sets.train.labels[poisoned_train_index])\n",
    "        polluted_inception_feature = full_model.generate_inception_features(poisoned_dataset, None)\n",
    "        polluted_inception_features.append(polluted_inception_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Initialize two top models firstly then compute the final results\n",
    "\n",
    "C = 1.0 / (len(data_sets.train.x) * .001)  \n",
    "if num_classes == 2:\n",
    "    log_reg_model = LogisticRegression(\n",
    "                C=C,\n",
    "                tol=1e-8,\n",
    "                fit_intercept=False, \n",
    "                solver='lbfgs',\n",
    "                warm_start=True, #True\n",
    "                max_iter=1000)\n",
    "else:\n",
    "    log_reg_model = LogisticRegression(\n",
    "                C=C,\n",
    "                tol=1e-8,\n",
    "                fit_intercept=False, \n",
    "                solver='lbfgs',\n",
    "                multi_class='multinomial',\n",
    "                warm_start=True, #True\n",
    "                max_iter=1000) \n",
    "    \n",
    "svc_model = SVC(\n",
    "                C=C,\n",
    "                kernel='linear', \n",
    "                probability = True\n",
    "                )\n",
    "\n",
    "logits_log_reg, logits_svc_model = [], []\n",
    "for polluted_inception_feature, poisoned_train_index, target_test_index in zip(polluted_inception_features, poisoned_train_indices, target_test_indices):\n",
    "    \n",
    "    X, Y = np.copy(clean_inception_features), np.copy(data_sets.train.labels)\n",
    "    X[poisoned_train_index] = polluted_inception_feature\n",
    "    target_test_dataset = DataSet(np.copy(data_sets.test.x[[target_test_index]]), np.copy(data_sets.test.labels[[target_test_index]]))\n",
    "    with full_graph.as_default():\n",
    "        target_test_inception_feature = full_model.generate_inception_features(target_test_dataset, None)\n",
    "    \n",
    "    log_reg_model.fit(X, Y)\n",
    "    svc_model.fit(X, Y)\n",
    "    \n",
    "    logits_log_reg.append(log_reg_model.predict_proba(target_test_inception_feature)[0][int(data_sets.test.labels[target_test_index])])\n",
    "    logits_svc_model.append(svc_model.predict_proba(target_test_inception_feature)[0][int(data_sets.test.labels[target_test_index])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0012622251352524406,\n",
       " 0.7417485162474685,\n",
       " 0.008847530885971025,\n",
       " 5.446869043100385e-09,\n",
       " 1.3958972658359296e-06,\n",
       " 0.0036751563655723117,\n",
       " 0.44310249340949986]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_svc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.026601219866971445,\n",
       " 0.9733608682617317,\n",
       " 0.028525896055540123,\n",
       " 0.0023432224655087763,\n",
       " 0.018794977291813564,\n",
       " 0.012494134339237295,\n",
       " 0.6688961455981225]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:influence_env]",
   "language": "python",
   "name": "conda-env-influence_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
