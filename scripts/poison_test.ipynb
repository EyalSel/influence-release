{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import IPython\n",
    "import copy\n",
    "from shutil import copyfile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "\n",
    "import sys\n",
    "sys.dont_write_bytecode=True\n",
    "\n",
    "PACKAGE_PARENT = '../'\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser('__file__'))))\n",
    "sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
    "\n",
    "from influence.inceptionModel import BinaryInceptionModel\n",
    "from influence.binaryLogisticRegressionWithLBFGS import BinaryLogisticRegressionWithLBFGS\n",
    "import influence.experiments\n",
    "from influence.dataset import DataSet\n",
    "# from influence.dataset_poisoning import iterative_attack, select_examples_to_attack, get_projection_to_box_around_orig_point, generate_inception_features\n",
    "from influence.iter_attack import iterative_attack, select_examples_to_attack, get_projection_to_box_around_orig_point, generate_inception_features\n",
    "from influence.Progress import *\n",
    "\n",
    "from load_animals import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Koda from disk...\n"
     ]
    }
   ],
   "source": [
    "img_side = 299\n",
    "num_channels = 3\n",
    " \n",
    "initial_learning_rate = 0.001 \n",
    "keep_probs = None\n",
    "decay_epochs = [1000, 10000]\n",
    "\n",
    "weight_decay = 0.001\n",
    "\n",
    "num_classes = 2\n",
    "max_lbfgs_iter = 1000\n",
    "\n",
    "num_train_ex = 400 #900\n",
    "num_test_ex = 200 #300\n",
    "batch_size = 100\n",
    "\n",
    "dataset_name = 'dogfish_%s_%s' % (num_train_ex, num_test_ex)\n",
    "# data_sets = load_animals(\n",
    "#     num_train_ex_per_class=num_train_ex_per_class, \n",
    "#     num_test_ex_per_class=num_test_ex_per_class,\n",
    "#     classes=['dog', 'fish'])\n",
    "# data_sets = load_dogfish_with_orig_and_koda()\n",
    "# X, Y = load_koda()\n",
    "# print(X.shape, Y.shape)\n",
    "data_sets = new_load_dogfish_with_koda(num_train_ex, num_test_ex)\n",
    "\n",
    "full_graph = tf.Graph()\n",
    "top_graph = tf.Graph()\n",
    "\n",
    "# dummy_data_sets = load_dummy(num_train_ex_per_class=num_train_ex_per_class, \n",
    "#     num_test_ex_per_class=num_test_ex_per_class,\n",
    "#     classes=['dog', 'fish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[genericNeuralNet.py:193 -   get_vec_to_list_fn() ] Total number of parameters: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Top:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[genericNeuralNet.py:193 -   get_vec_to_list_fn() ] Total number of parameters: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.00832113\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022926298\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.02760271\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 3.738009e-07\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4723186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [46] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: [0.01466117]\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: [0.00863267]\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: [0.0210622]\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  [0.9975]\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   [0.99]\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.02853913\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4723186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating poisoned dataset...\n",
      "****** Attacking test_idx 3 ******\n",
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.018769\n",
      "         Iterations: 6\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 76\n",
      "         Hessian evaluations: 41\n",
      "Inverse HVP took 0.53383398056 sec\n",
      "Inverse HVP took 0.000648975372314 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:113 -     iterative_attack() ] Test idx: [3], Indices to poison: [219], Labels subset: [0.]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiplying by 400 train examples took 37.6183030605 sec\n",
      "finished calculating grad_wrt_input_val\n",
      "\n",
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -3.073457\n",
      "         Iterations: 6\n",
      "         Function evaluations: 67\n",
      "         Gradient evaluations: 62\n",
      "         Hessian evaluations: 28\n",
      "Inverse HVP took 0.401269197464 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.0673271\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000924825668335 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 0 perturbation shape: (1, 268203), perturbation: [[ 0.00277827 -0.00508411  0.00018054 ...  0.0001965  -0.0051954\n",
      "   0.00426188]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 19.45317268371582, mean: -0.00013779827949543007, min: -15.860267639160156\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008418067\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0023011917\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.030149236\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.985\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00079421885\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.497678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [24] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9928814e-01 7.1188918e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.99895835 0.00104164]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -3.504743\n",
      "         Iterations: 8\n",
      "         Function evaluations: 22\n",
      "         Gradient evaluations: 29\n",
      "         Hessian evaluations: 43\n",
      "Inverse HVP took 0.240314006805 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06539903\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000937938690186 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 1 perturbation shape: (1, 268203), perturbation: [[ 0.00152116 -0.01838516 -0.00743758 ...  0.00687484  0.00232307\n",
      "  -0.0065069 ]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 15.517000198364258, mean: 0.00018884473326482554, min: -14.932860374450684\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.0084151365\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0023006764\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.029481094\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0007843431\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4969876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [30] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9928838e-01 7.1168056e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.9989623  0.00103774]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -3.403970\n",
      "         Iterations: 8\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 95\n",
      "         Hessian evaluations: 40\n",
      "Inverse HVP took 0.599786043167 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06534034\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000959157943726 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 2 perturbation shape: (1, 268203), perturbation: [[-0.00095023 -0.01014559 -0.00079266 ...  0.00454687  0.00294159\n",
      "  -0.00598442]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 13.082998275756836, mean: 0.00017843120204427237, min: -12.10988998413086\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.00841327\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0023003728\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.02892821\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.000779485\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4965405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [26] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.992867e-01 7.133728e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.9989641  0.00103588]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -3.316508\n",
      "         Iterations: 6\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 77\n",
      "         Hessian evaluations: 34\n",
      "Inverse HVP took 0.485736846924 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06518982\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000942230224609 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 3 perturbation shape: (1, 268203), perturbation: [[-2.20682425e-03 -1.25852991e-02 -5.90661075e-04 ...  7.43870111e-03\n",
      "  -6.04597293e-03 -7.74188084e-05]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 13.738391876220703, mean: 0.0001350334715138666, min: -10.647239685058594\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008414633\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.002300755\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.028439246\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00078802695\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4968204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [23] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.992848e-01 7.152379e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.9989417  0.00105835]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -3.234814\n",
      "         Iterations: 6\n",
      "         Function evaluations: 60\n",
      "         Gradient evaluations: 55\n",
      "         Hessian evaluations: 34\n",
      "Inverse HVP took 0.389356851578 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06501718\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.00101089477539 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 4 perturbation shape: (1, 268203), perturbation: [[-0.00046888 -0.01026127  0.00092678 ...  0.00685395 -0.00534653\n",
      "   0.00145253]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 13.489974975585938, mean: 9.88928334470611e-05, min: -10.570782661437988\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008416891\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.00230141\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.027988458\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0008007083\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4972792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [28] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.992824e-01 7.176074e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.9989121  0.00108793]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -3.157461\n",
      "         Iterations: 8\n",
      "         Function evaluations: 194\n",
      "         Gradient evaluations: 188\n",
      "         Hessian evaluations: 52\n",
      "Inverse HVP took 1.08045315742 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06483548\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000926971435547 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 5 perturbation shape: (1, 268203), perturbation: [[-3.10568372e-03 -5.55926701e-03  3.28794797e-03 ...  7.82522897e-04\n",
      "  -2.01923121e-03 -7.04642443e-05]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 12.597513198852539, mean: 7.525887415501056e-05, min: -9.152861595153809\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008417225\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.002301567\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.02757218\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0008068018\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4973295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.992786e-01 7.214559e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.99888724 0.00111278]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -3.082281\n",
      "         Iterations: 9\n",
      "         Function evaluations: 154\n",
      "         Gradient evaluations: 150\n",
      "         Hessian evaluations: 42\n",
      "Inverse HVP took 0.879534006119 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06469465\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000945091247559 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 6 perturbation shape: (1, 268203), perturbation: [[-0.00235342 -0.00110079  0.00522537 ... -0.00111981 -0.0003452\n",
      "   0.00206257]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 9.832306861877441, mean: 5.28452749064012e-05, min: -9.03994083404541\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.00841534\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.002301233\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.027193937\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00080106145\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4968867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [22] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9927348e-01 7.2648335e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.9988826  0.00111746]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -3.012445\n",
      "         Iterations: 7\n",
      "         Function evaluations: 75\n",
      "         Gradient evaluations: 70\n",
      "         Hessian evaluations: 44\n",
      "Inverse HVP took 0.502628087997 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.064520456\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000939130783081 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 7 perturbation shape: (1, 268203), perturbation: [[-0.00287021 -0.00323618  0.00177928 ...  0.00129193 -0.0009803\n",
      "   0.00190307]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 8.242362976074219, mean: 3.904415000545279e-05, min: -7.917536735534668\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.0084135905\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0023008774\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.026840884\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0007970198\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4964879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [23] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9926764e-01 7.3231448e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.99888176 0.00111825]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.946622\n",
      "         Iterations: 8\n",
      "         Function evaluations: 89\n",
      "         Gradient evaluations: 85\n",
      "         Hessian evaluations: 43\n",
      "Inverse HVP took 0.551967859268 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06439937\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000936031341553 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 8 perturbation shape: (1, 268203), perturbation: [[ 1.41542405e-05 -3.54771316e-03  1.31447299e-03 ... -1.26760267e-03\n",
      "  -2.63049209e-04  3.73891275e-03]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 6.616013526916504, mean: 4.844688177372159e-05, min: -7.963659286499023\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008411642\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0023004974\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.026496598\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0007936473\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4960392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [27] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9926013e-01 7.3989545e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.99888664 0.00111342]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.881608\n",
      "         Iterations: 8\n",
      "         Function evaluations: 71\n",
      "         Gradient evaluations: 67\n",
      "         Hessian evaluations: 40\n",
      "Inverse HVP took 0.449588060379 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06433531\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000952005386353 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 9 perturbation shape: (1, 268203), perturbation: [[-0.00061178 -0.00156601  0.00191934 ... -0.00705115  0.00211344\n",
      "   0.00325313]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 6.80500602722168, mean: 4.9825670686391174e-05, min: -6.901066303253174\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008408311\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022998317\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.026177058\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0007846581\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4952762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [24] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9925154e-01 7.4837444e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.9988972  0.00110273]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.819881\n",
      "         Iterations: 6\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 90\n",
      "         Hessian evaluations: 34\n",
      "Inverse HVP took 0.55491399765 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06425202\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000926971435547 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 10 perturbation shape: (1, 268203), perturbation: [[-0.002265    0.00094262  0.00549987 ...  0.00464717 -0.00567902\n",
      "   0.00428397]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.889382362365723, mean: 7.810719626640222e-05, min: -10.195648193359375\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008404598\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.002298986\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.025883852\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00077257695\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.494456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [25] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9924266e-01 7.5731205e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.99891067 0.00108931]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.762608\n",
      "         Iterations: 9\n",
      "         Function evaluations: 23\n",
      "         Gradient evaluations: 31\n",
      "         Hessian evaluations: 43\n",
      "Inverse HVP took 0.260893821716 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06420543\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000921010971069 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 11 perturbation shape: (1, 268203), perturbation: [[-0.0023261   0.00011793  0.00291006 ... -0.0012289  -0.00600435\n",
      "   0.00641387]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 6.658056259155273, mean: -1.6448451789995852e-05, min: -9.1572265625\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008404557\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022990978\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.025578253\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00077765825\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4944124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9923515e-01 7.6482934e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.99888855 0.00111146]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.702124\n",
      "         Iterations: 6\n",
      "         Function evaluations: 85\n",
      "         Gradient evaluations: 80\n",
      "         Hessian evaluations: 34\n",
      "Inverse HVP took 0.498571157455 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06422659\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.00093412399292 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 12 perturbation shape: (1, 268203), perturbation: [[-0.00078688 -0.00191363  0.00363771 ... -0.00243135 -0.00077667\n",
      "   0.00498624]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 6.120872974395752, mean: 6.280870405548871e-05, min: -9.453763008117676\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008398119\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022976722\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.025356624\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0007525283\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4929783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [29] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9922967e-01 7.7032403e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.9989273  0.00107265]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.657580\n",
      "         Iterations: 7\n",
      "         Function evaluations: 79\n",
      "         Gradient evaluations: 74\n",
      "         Hessian evaluations: 35\n",
      "Inverse HVP took 0.497657060623 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06424558\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000935077667236 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 13 perturbation shape: (1, 268203), perturbation: [[ 0.00146615 -0.00585419 -0.0061822  ... -0.01000616 -0.00122219\n",
      "   0.00744197]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.636945724487305, mean: 2.1948463015536743e-05, min: -8.120898246765137\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008396938\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022974913\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.02509563\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0007508823\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4926922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.992242e-01 7.758408e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.99892247 0.00107758]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.604840\n",
      "         Iterations: 6\n",
      "         Function evaluations: 73\n",
      "         Gradient evaluations: 67\n",
      "         Hessian evaluations: 34\n",
      "Inverse HVP took 0.472622156143 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06432243\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000917196273804 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 14 perturbation shape: (1, 268203), perturbation: [[-6.41416409e-06 -8.79178639e-04 -1.93878938e-03 ... -4.09477297e-03\n",
      "  -1.60081254e-03  1.01703675e-02]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.637391567230225, mean: 3.295504471164394e-05, min: -9.902958869934082\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.0083896415\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022959502\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.024957128\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0007176602\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.491043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [26] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9921906e-01 7.8097160e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.9989712 0.0010288]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.574913\n",
      "         Iterations: 10\n",
      "         Function evaluations: 29\n",
      "         Gradient evaluations: 38\n",
      "         Hessian evaluations: 44\n",
      "Inverse HVP took 0.290824890137 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06448749\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000929117202759 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 15 perturbation shape: (1, 268203), perturbation: [[ 1.98964844e-03 -9.64328647e-05 -1.10221526e-03 ... -1.55167300e-02\n",
      "   5.01920516e-03  1.20228091e-02]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.722766399383545, mean: 6.315450381662625e-05, min: -8.745357513427734\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008386031\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022951409\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.024793906\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0007011337\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4902408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [24] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9921525e-01 7.8477437e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.99899715 0.00100287]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.541533\n",
      "         Iterations: 6\n",
      "         Function evaluations: 87\n",
      "         Gradient evaluations: 83\n",
      "         Hessian evaluations: 34\n",
      "Inverse HVP took 0.51270198822 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06451477\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000962972640991 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 16 perturbation shape: (1, 268203), perturbation: [[ 0.00084247  0.00222213 -0.00016245 ...  0.00194425 -0.00280301\n",
      "   0.00499902]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.976527214050293, mean: 2.5449076226366896e-06, min: -9.226846694946289\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008386099\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022953348\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.024566336\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00070402643\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4902046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9920976e-01 7.9023599e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.99899036 0.00100965]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.492300\n",
      "         Iterations: 8\n",
      "         Function evaluations: 89\n",
      "         Gradient evaluations: 85\n",
      "         Hessian evaluations: 44\n",
      "Inverse HVP took 0.583163022995 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06452485\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.00105595588684 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 17 perturbation shape: (1, 268203), perturbation: [[ 0.00245127 -0.00456908 -0.00703355 ... -0.00926321  0.00662756\n",
      "   0.01149317]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.720062255859375, mean: 0.00010545831823819765, min: -7.09053897857666\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.00838267\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022945849\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.024411982\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00068920304\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4894369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [23] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.992041e-01 7.958873e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9901986e-01 9.8011515e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -2.459867\n",
      "         Iterations: 9\n",
      "         Function evaluations: 17\n",
      "         Gradient evaluations: 25\n",
      "         Hessian evaluations: 41\n",
      "Inverse HVP took 0.218712091446 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.064606026\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000926971435547 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 18 perturbation shape: (1, 268203), perturbation: [[-0.00101254 -0.000655   -0.00236866 ... -0.00898122 -0.00014348\n",
      "   0.00843897]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 7.0228800773620605, mean: -2.2560387157989432e-05, min: -6.7569169998168945\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008384143\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022950491\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.024161957\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00069979136\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4897258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [22] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991992e-01 8.008243e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.990024e-01 9.976665e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.407474\n",
      "         Iterations: 5\n",
      "         Function evaluations: 42\n",
      "         Gradient evaluations: 36\n",
      "         Hessian evaluations: 32\n",
      "Inverse HVP took 0.296252012253 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06457982\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000914096832275 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 19 perturbation shape: (1, 268203), perturbation: [[ 0.00167516 -0.00245735 -0.00073005 ... -0.00936558  0.01151259\n",
      "   0.00569454]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 6.1057562828063965, mean: 2.5952925274212367e-05, min: -5.20851993560791\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008384177\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.002295018\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.023934282\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00070208113\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4897451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9919564e-01 8.0440671e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9900407e-01 9.9589722e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.360557\n",
      "         Iterations: 8\n",
      "         Function evaluations: 213\n",
      "         Gradient evaluations: 205\n",
      "         Hessian evaluations: 46\n",
      "Inverse HVP took 1.19158196449 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06458438\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000931024551392 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 20 perturbation shape: (1, 268203), perturbation: [[-0.00134777 -0.0022487  -0.00193692 ... -0.00969601  0.00047174\n",
      "   0.00837669]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 6.359922409057617, mean: -8.830375994177057e-06, min: -6.450091361999512\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008383109\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.002294895\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.023746204\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0007003139\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4894733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9919075e-01 8.0924679e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.990075e-01 9.925546e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.320189\n",
      "         Iterations: 8\n",
      "         Function evaluations: 76\n",
      "         Gradient evaluations: 72\n",
      "         Hessian evaluations: 44\n",
      "Inverse HVP took 0.494605064392 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.064560376\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000907897949219 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 21 perturbation shape: (1, 268203), perturbation: [[ 0.00062556 -0.00203281  0.00266964 ... -0.0090481   0.00822051\n",
      "   0.00616683]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 6.124391555786133, mean: 0.00012565783195362293, min: -5.515191078186035\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008382907\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022947937\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.023537153\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00070012343\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.489445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9918741e-01 8.1259815e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9900997e-01 9.9006074e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.277208\n",
      "         Iterations: 6\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 117\n",
      "         Hessian evaluations: 41\n",
      "Inverse HVP took 0.707736968994 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06455935\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.00088906288147 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 22 perturbation shape: (1, 268203), perturbation: [[-0.00457989 -0.00247415  0.00195029 ... -0.00991751  0.00307369\n",
      "   0.00551734]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.097304821014404, mean: 2.1040463305781e-05, min: -4.6804351806640625\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008383281\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022950168\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.02333691\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0007032152\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4894886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [9] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9918360e-01 8.1635057e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.990075e-01 9.925115e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.235601\n",
      "         Iterations: 5\n",
      "         Function evaluations: 83\n",
      "         Gradient evaluations: 76\n",
      "         Hessian evaluations: 32\n",
      "Inverse HVP took 0.491176128387 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06457054\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000920057296753 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 23 perturbation shape: (1, 268203), perturbation: [[-1.76903873e-03 -2.62391986e-03 -8.08247132e-05 ...  1.26036757e-03\n",
      "   5.29663963e-03  2.18541804e-03]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 4.924468994140625, mean: 0.00012516780458633877, min: -4.462336540222168\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008384042\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022952238\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.02312414\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.99\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.000710409\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4896467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [20] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991799e-01 8.200610e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.990062e-01 9.937822e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.192225\n",
      "         Iterations: 5\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 70\n",
      "         Hessian evaluations: 32\n",
      "Inverse HVP took 0.449080944061 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06457783\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000918865203857 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 24 perturbation shape: (1, 268203), perturbation: [[-0.00270682  0.00092665  0.00609457 ... -0.00604985  0.00213809\n",
      "   0.00144511]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 4.696721076965332, mean: 2.649914647565849e-05, min: -4.150676250457764\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008384399\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022953367\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022927685\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0007145798\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4897168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [10] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991761e-01 8.238440e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.990096e-01 9.903569e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.150755\n",
      "         Iterations: 5\n",
      "         Function evaluations: 60\n",
      "         Gradient evaluations: 54\n",
      "         Hessian evaluations: 32\n",
      "Inverse HVP took 0.385676145554 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.064598516\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000915050506592 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 25 perturbation shape: (1, 268203), perturbation: [[-0.00501764 -0.00086054  0.00363715 ...  0.00421368  0.00600074\n",
      "   0.00279248]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.600405693054199, mean: 0.00013233431096702135, min: -4.083993911743164\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008385396\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022956198\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022753624\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00072264636\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4899216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [15] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9917430e-01 8.2571176e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[0.9989999  0.00100018]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.115037\n",
      "         Iterations: 6\n",
      "         Function evaluations: 83\n",
      "         Gradient evaluations: 78\n",
      "         Hessian evaluations: 37\n",
      "Inverse HVP took 0.505714178085 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06461853\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000925064086914 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 26 perturbation shape: (1, 268203), perturbation: [[-0.00189989  0.00173585  0.0062424  ... -0.00931335  0.00244792\n",
      "   0.00312421]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 4.896503448486328, mean: 6.7404333201738e-05, min: -3.85817813873291\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.0083795525\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022942086\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022812454\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00069029885\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4886515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [20] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991703e-01 8.297015e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9904996e-01 9.5004088e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.122057\n",
      "         Iterations: 7\n",
      "         Function evaluations: 87\n",
      "         Gradient evaluations: 83\n",
      "         Hessian evaluations: 45\n",
      "Inverse HVP took 0.537229776382 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06462492\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.00093412399292 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 27 perturbation shape: (1, 268203), perturbation: [[-0.00365008 -0.0012861   0.00197499 ... -0.00044423  0.0034947\n",
      "   0.00736084]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.060031890869141, mean: 2.9794516330872256e-05, min: -4.750524997711182\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008384837\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022956736\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022554161\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0007229347\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4897463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [12] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991715e-01 8.285828e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.990073e-01 9.927024e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.071514\n",
      "         Iterations: 6\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 77\n",
      "         Hessian evaluations: 37\n",
      "Inverse HVP took 0.498764038086 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.064761326\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000944852828979 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 28 perturbation shape: (1, 268203), perturbation: [[-0.00355128 -0.00065914  0.00401346 ... -0.00786422  0.00269048\n",
      "   0.00178643]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 4.695640563964844, mean: 5.7446125832387314e-05, min: -4.5855712890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008376515\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022935863\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022743277\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0006749667\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4879591\n",
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991690e-01 8.310268e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9907720e-01 9.2281925e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [25] iter.\n",
      "After training with LBFGS: \n",
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.104106\n",
      "         Iterations: 7\n",
      "         Function evaluations: 92\n",
      "         Gradient evaluations: 87\n",
      "         Hessian evaluations: 49\n",
      "Inverse HVP took 0.600626945496 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06473764\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000917911529541 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 29 perturbation shape: (1, 268203), perturbation: [[-0.00155224 -0.00302511  0.00431393 ... -0.00042063  0.00471295\n",
      "   0.0054924 ]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.757188320159912, mean: 6.66832039047726e-05, min: -6.758675575256348\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.0083836485\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022954969\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022441305\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0007181656\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4894555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [12] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991720e-01 8.280073e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.990208e-01 9.792206e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.046144\n",
      "         Iterations: 8\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 21\n",
      "         Hessian evaluations: 45\n",
      "Inverse HVP took 0.210990905762 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.0648815\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000898838043213 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 30 perturbation shape: (1, 268203), perturbation: [[ 0.00093827  0.00263741  0.00319541 ... -0.00282338 -0.00028674\n",
      "   0.00139822]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.3038835525512695, mean: 5.368706908141903e-05, min: -5.151546478271484\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.00837491\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022933208\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022666091\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0006682518\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.487575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [29] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9916852e-01 8.3151355e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9909067e-01 9.0931414e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.084863\n",
      "         Iterations: 7\n",
      "         Function evaluations: 118\n",
      "         Gradient evaluations: 113\n",
      "         Hessian evaluations: 45\n",
      "Inverse HVP took 0.729308843613 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06488607\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.00092887878418 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 31 perturbation shape: (1, 268203), perturbation: [[-0.00069953 -0.00382115  0.004126   ... -0.00469232  0.00970304\n",
      "   0.00390277]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.763182163238525, mean: 9.066270651988611e-05, min: -5.75053596496582\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008381516\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.002295034\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.02236599\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00070607313\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4889774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [12] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991709e-01 8.291360e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9903846e-01 9.6162793e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.026830\n",
      "         Iterations: 9\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 79\n",
      "         Hessian evaluations: 47\n",
      "Inverse HVP took 0.52405500412 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06495593\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000931024551392 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 32 perturbation shape: (1, 268203), perturbation: [[-0.00227425 -0.00078356  0.00189168 ... -0.00336496  0.0012143\n",
      "   0.0015604 ]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 3.987032651901245, mean: 4.589510561199277e-05, min: -4.756896018981934\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008373651\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.002293058\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022583585\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0006610146\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.487289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [24] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9916875e-01 8.3124492e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9910176e-01 8.9824869e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.065433\n",
      "         Iterations: 8\n",
      "         Function evaluations: 21\n",
      "         Gradient evaluations: 28\n",
      "         Hessian evaluations: 43\n",
      "Inverse HVP took 0.253458976746 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.064982556\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000935077667236 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 33 perturbation shape: (1, 268203), perturbation: [[-0.00196621 -0.00177558  0.00530981 ... -0.00500235  0.00301005\n",
      "   0.00552752]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.534269332885742, mean: 4.327869893237477e-05, min: -7.3308539390563965\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008382937\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022954524\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.02221055\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00071641663\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4892647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [12] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991703e-01 8.296710e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9902761e-01 9.7243185e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -1.994670\n",
      "         Iterations: 7\n",
      "         Function evaluations: 80\n",
      "         Gradient evaluations: 76\n",
      "         Hessian evaluations: 46\n",
      "Inverse HVP took 0.520635128021 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06504967\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000910997390747 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 34 perturbation shape: (1, 268203), perturbation: [[-0.00242585 -0.00047553  0.00171037 ...  0.00846933  0.00551153\n",
      "   0.00205087]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 4.487094402313232, mean: 0.00014474285939208075, min: -5.228367805480957\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.00837197\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022927276\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022558918\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0006498888\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4869022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [25] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9916911e-01 8.3087117e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9911505e-01 8.8495284e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.057468\n",
      "         Iterations: 8\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 132\n",
      "         Hessian evaluations: 43\n",
      "Inverse HVP took 0.777717828751 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06506693\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000943899154663 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 35 perturbation shape: (1, 268203), perturbation: [[-0.00161325 -0.00201405  0.00555449 ... -0.0050364  -0.00134711\n",
      "   0.00400786]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.609014511108398, mean: -2.6006885898531943e-05, min: -7.467685699462891\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008381452\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022951728\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022167563\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0007112764\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4889195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [12] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991696e-01 8.304113e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9903846e-01 9.6151925e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -1.982961\n",
      "         Iterations: 7\n",
      "         Function evaluations: 112\n",
      "         Gradient evaluations: 107\n",
      "         Hessian evaluations: 45\n",
      "Inverse HVP took 0.704758882523 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06514318\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000908136367798 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 36 perturbation shape: (1, 268203), perturbation: [[-0.00279301 -0.00164291  0.00099456 ... -0.00274281  0.00904508\n",
      "   0.00390566]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.616703987121582, mean: 9.795773563176138e-05, min: -5.203678607940674\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008371204\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022926005\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022496464\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0006480173\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4867187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [23] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9916840e-01 8.3167525e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.991211e-01 8.788409e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.042284\n",
      "         Iterations: 7\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 77\n",
      "         Hessian evaluations: 40\n",
      "Inverse HVP took 0.508765935898 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06518538\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000930070877075 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 37 perturbation shape: (1, 268203), perturbation: [[-0.00049769 -0.00149917  0.00185803 ... -0.01102895  0.00174274\n",
      "   0.00660569]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.162078857421875, mean: 3.4710243592517954e-05, min: -6.133983612060547\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.00837988\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022948266\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022126844\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0006993865\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4885683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [12] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9916863e-01 8.3136931e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9905139e-01 9.4862224e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -1.972383\n",
      "         Iterations: 8\n",
      "         Function evaluations: 171\n",
      "         Gradient evaluations: 163\n",
      "         Hessian evaluations: 54\n",
      "Inverse HVP took 1.01321721077 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.065158136\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000932931900024 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 38 perturbation shape: (1, 268203), perturbation: [[-0.00294253 -0.00396212  0.00174692 ...  0.0089522   0.00356859\n",
      "   0.00097561]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 4.915643692016602, mean: 0.00011834076310293107, min: -5.135420322418213\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.00837016\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.002292413\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.02245865\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0006418957\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4864733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [26] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9916756e-01 8.3243271e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9912828e-01 8.7168155e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.031933\n",
      "         Iterations: 5\n",
      "         Function evaluations: 79\n",
      "         Gradient evaluations: 73\n",
      "         Hessian evaluations: 30\n",
      "Inverse HVP took 0.452775001526 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06526764\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000944852828979 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 39 perturbation shape: (1, 268203), perturbation: [[-0.00182847 -0.00032226  0.00126951 ... -0.00072319 -0.00195901\n",
      "   0.00364394]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 4.792912483215332, mean: 1.9522853376602846e-05, min: -5.928725242614746\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.0083779665\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022944126\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022115603\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0006892951\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4881384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [12] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991673e-01 8.326676e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9906808e-01 9.3197386e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -1.967032\n",
      "         Iterations: 8\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 16\n",
      "         Hessian evaluations: 39\n",
      "Inverse HVP took 0.174708843231 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.0652111\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000933885574341 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 40 perturbation shape: (1, 268203), perturbation: [[-0.00431681 -0.00444508  0.00207363 ... -0.00525716  0.00938477\n",
      "   0.00377727]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.145407676696777, mean: 0.00011336276824891368, min: -5.592601299285889\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.0083692875\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022922351\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022424325\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00063621555\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4862735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [25] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9916673e-01 8.3330896e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9913639e-01 8.6362637e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.023059\n",
      "         Iterations: 6\n",
      "         Function evaluations: 71\n",
      "         Gradient evaluations: 65\n",
      "         Hessian evaluations: 35\n",
      "Inverse HVP took 0.440387010574 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06529345\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000924110412598 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 41 perturbation shape: (1, 268203), perturbation: [[ 0.00031638  0.00088204  0.00251777 ... -0.01619079  0.00202835\n",
      "   0.00799225]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.8756513595581055, mean: 2.465096470615163e-05, min: -7.007684230804443\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008377642\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.002294358\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022060843\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00068598706\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4880607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [12] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991671e-01 8.329278e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9906904e-01 9.3094120e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -1.954718\n",
      "         Iterations: 5\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 82\n",
      "         Hessian evaluations: 30\n",
      "Inverse HVP took 0.502400159836 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06526153\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000998020172119 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 42 perturbation shape: (1, 268203), perturbation: [[-0.00614368 -0.00363136  0.0021041  ...  0.00987682  0.00165421\n",
      "  -0.00019669]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 4.897738456726074, mean: 0.00010677030428572503, min: -5.840454578399658\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008367182\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022918181\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.02246338\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00062309584\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.485789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [25] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9916637e-01 8.3366875e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.991504e-01 8.496082e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.027588\n",
      "         Iterations: 5\n",
      "         Function evaluations: 62\n",
      "         Gradient evaluations: 56\n",
      "         Hessian evaluations: 30\n",
      "Inverse HVP took 0.373955011368 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.065361\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000936031341553 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 43 perturbation shape: (1, 268203), perturbation: [[-0.0004221   0.00235438  0.00305514 ... -0.00216253 -0.00287015\n",
      "   0.00501273]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 6.8897809982299805, mean: 3.2315645493585495e-05, min: -8.325881958007812\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.00837678\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022942259\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022042226\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00068436615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [12] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4878516\n",
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9916685e-01 8.3319459e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.990748e-01 9.251513e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -1.949174\n",
      "         Iterations: 9\n",
      "         Function evaluations: 143\n",
      "         Gradient evaluations: 139\n",
      "         Hessian evaluations: 61\n",
      "Inverse HVP took 0.861176013947 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06532704\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000935077667236 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 44 perturbation shape: (1, 268203), perturbation: [[-5.94748929e-03 -1.10292155e-03  5.41904010e-05 ... -4.56581637e-03\n",
      "   8.55667144e-03  3.34895262e-03]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.762350082397461, mean: 0.00010789050828395616, min: -5.349908828735352\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008366475\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022916966\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022444356\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.000617297\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4856207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [25] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9916625e-01 8.3374733e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9915636e-01 8.4364251e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.021856\n",
      "         Iterations: 9\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 88\n",
      "         Hessian evaluations: 48\n",
      "Inverse HVP took 0.5734770298 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06539241\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000913858413696 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 45 perturbation shape: (1, 268203), perturbation: [[ 0.00266451 -0.00225164  0.00075225 ... -0.01709362  0.003318\n",
      "   0.0103121 ]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 6.598849296569824, mean: 5.008385517110274e-05, min: -7.706424713134766\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008375659\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022939784\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022029782\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0006768229\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.487601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [12] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9916720e-01 8.3286996e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.990841e-01 9.159323e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -1.945032\n",
      "         Iterations: 8\n",
      "         Function evaluations: 83\n",
      "         Gradient evaluations: 80\n",
      "         Hessian evaluations: 44\n",
      "Inverse HVP took 0.559130907059 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06535419\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000909090042114 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 46 perturbation shape: (1, 268203), perturbation: [[-0.00722432  0.0013379   0.00124049 ...  0.01098902  0.0010151\n",
      "  -0.00194401]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 4.265033721923828, mean: 8.577362138855817e-05, min: -5.124311447143555\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008365711\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022915388\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022436408\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0006130195\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4854472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [25] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991653e-01 8.347526e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9916375e-01 8.3630311e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.018519\n",
      "         Iterations: 6\n",
      "         Function evaluations: 89\n",
      "         Gradient evaluations: 83\n",
      "         Hessian evaluations: 38\n",
      "Inverse HVP took 0.537266969681 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06543572\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000913143157959 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 47 perturbation shape: (1, 268203), perturbation: [[ 0.0011514  -0.00138377  0.00177922 ... -0.0004868  -0.00612974\n",
      "   0.00546848]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 6.9017133712768555, mean: 4.683414750268387e-05, min: -8.184103965759277\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008374724\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022937765\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.02202562\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00067057926\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4873905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [12] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991672e-01 8.328072e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9909210e-01 9.0792036e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -1.942774\n",
      "         Iterations: 7\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 74\n",
      "         Hessian evaluations: 41\n",
      "Inverse HVP took 0.509612083435 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06537684\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000962972640991 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 48 perturbation shape: (1, 268203), perturbation: [[-0.00340791 -0.0005145   0.00098994 ... -0.00666363  0.01023264\n",
      "   0.00356182]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.146049499511719, mean: 8.158454821581547e-05, min: -5.043200492858887\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008365195\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022914228\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022420911\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0006090708\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.485333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [24] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9916494e-01 8.3509821e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.991679e-01 8.321465e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.014156\n",
      "         Iterations: 7\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 72\n",
      "         Hessian evaluations: 41\n",
      "Inverse HVP took 0.482112169266 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06544878\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.00110507011414 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 49 perturbation shape: (1, 268203), perturbation: [[ 0.00104236 -0.00241511  0.00052739 ... -0.00494073 -0.00070413\n",
      "   0.00230272]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 6.230921745300293, mean: -1.2193355090607309e-05, min: -7.82052755355835\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008374067\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022936116\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.02200966\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0006656073\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4872496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [12] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991666e-01 8.333851e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9909806e-01 9.0192445e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -1.938314\n",
      "         Iterations: 7\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 114\n",
      "         Hessian evaluations: 44\n",
      "Inverse HVP took 0.696670055389 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06539504\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000886917114258 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 50 perturbation shape: (1, 268203), perturbation: [[-0.00395827 -0.00510578  0.00024502 ... -0.00379296  0.00889473\n",
      "   0.0022236 ]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 3.9276483058929443, mean: 8.475807606846096e-05, min: -4.825111389160156\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008365219\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022914612\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.02238036\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00060946716\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4853284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [24] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.9916506e-01 8.3492120e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9916553e-01 8.3445030e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2.005105\n",
      "         Iterations: 6\n",
      "         Function evaluations: 84\n",
      "         Gradient evaluations: 78\n",
      "         Hessian evaluations: 34\n",
      "Inverse HVP took 0.504115104675 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06549826\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000887155532837 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 51 perturbation shape: (1, 268203), perturbation: [[-0.00086124 -0.00091009  0.00070064 ... -0.0053504  -0.00115635\n",
      "   0.00226698]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 6.402997970581055, mean: -1.0070520921562605e-05, min: -6.896027565002441\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008373188\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022934251\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022006841\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.0006614837\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4870508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [12] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991659e-01 8.341737e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9910420e-01 8.9582644e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -1.936407\n",
      "         Iterations: 8\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 16\n",
      "         Hessian evaluations: 38\n",
      "Inverse HVP took 0.17028594017 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.065415226\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000943899154663 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 52 perturbation shape: (1, 268203), perturbation: [[-0.00313931 -0.000199    0.00182678 ... -0.00741926  0.00817475\n",
      "   0.00131791]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.664578437805176, mean: 0.00010061999644005176, min: -4.683772087097168\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.00836507\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022914354\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022348065\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.000609456\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4852934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [25] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991647e-01 8.352717e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9916649e-01 8.3351776e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -1.998168\n",
      "         Iterations: 8\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 90\n",
      "         Hessian evaluations: 44\n",
      "Inverse HVP took 0.601258993149 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.065487966\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.00094199180603 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 53 perturbation shape: (1, 268203), perturbation: [[-0.0007661   0.00024316  0.00201384 ... -0.0032973   0.0029134\n",
      "   0.00310082]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 5.739777565002441, mean: -1.4026885063836714e-05, min: -5.401489734649658\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008372397\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.0022932657\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022000175\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.00065654935\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.4868703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [12] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991658e-01 8.342834e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9911195e-01 8.8802318e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -1.933585\n",
      "         Iterations: 7\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 84\n",
      "         Hessian evaluations: 40\n",
      "Inverse HVP took 0.541332960129 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Progress.py:231 - get_grad_of_influence_wrt_input() ] Looping over training points. Counter: 0, idx_to_remove: 219\n",
      "[genericNeuralNet.py:911 - grad_influence_wrt_input() ] calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of test gradient: 0.06544987\n",
      "Loaded inverse HVP from output/dogfish_400_200_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.000939130783081 sec\n",
      "Entering the for loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:142 -     iterative_attack() ] Attach_iter 54 perturbation shape: (1, 268203), perturbation: [[-0.00579451  0.00146253  0.00298796 ... -0.00924681  0.00729955\n",
      "   0.00270696]]\n",
      "[iter_attack.py:67 - poison_with_influence_proj_gradient_step() ] -- max: 4.919024467468262, mean: 0.00010703031239584766, min: -4.619388103485107\n",
      "[genericNeuralNet.py:330 -     print_model_eval() ] Train loss (w reg) on all data: 0.008364571\n",
      "[genericNeuralNet.py:331 -     print_model_eval() ] Train loss (w/o reg) on all data: 0.002291307\n",
      "[genericNeuralNet.py:333 -     print_model_eval() ] Test loss (w/o reg) on all data: 0.022337439\n",
      "[genericNeuralNet.py:334 -     print_model_eval() ] Train acc on all data:  1.0\n",
      "[genericNeuralNet.py:335 -     print_model_eval() ] Test acc on all data:   0.995\n",
      "[genericNeuralNet.py:337 -     print_model_eval() ] Norm of the mean of gradients: 0.000605233\n",
      "[genericNeuralNet.py:338 -     print_model_eval() ] Norm of the params: 3.485187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [24] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iter_attack.py:174 -     iterative_attack() ] Test pred (full): [[9.991646e-01 8.353787e-04]]\n",
      "[iter_attack.py:177 -     iterative_attack() ] Test pred (top): [[9.9916995e-01 8.3006796e-04]]\n",
      "[iter_attack.py:125 -     iterative_attack() ] *** Iter: 55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -1.994653\n",
      "         Iterations: 7\n",
      "         Function evaluations: 100\n",
      "         Gradient evaluations: 95\n",
      "         Hessian evaluations: 46\n",
      "Inverse HVP took 0.628654003143 sec\n"
     ]
    }
   ],
   "source": [
    "print('*** Full:')\n",
    "with full_graph.as_default():\n",
    "    full_model_name = '%s_inception_wd-%s' % (dataset_name, weight_decay)\n",
    "    full_model = BinaryInceptionModel(\n",
    "        img_side=img_side,\n",
    "        num_channels=num_channels,\n",
    "        weight_decay=weight_decay,\n",
    "        num_classes=num_classes, \n",
    "        batch_size=batch_size,\n",
    "        data_sets=data_sets,\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        keep_probs=keep_probs,\n",
    "        decay_epochs=decay_epochs,\n",
    "        mini_batch=True,\n",
    "        train_dir='output',\n",
    "        log_dir='log',\n",
    "        model_name=full_model_name)\n",
    "    \n",
    "    for data_set, label in [\n",
    "        (data_sets.train, 'train'),\n",
    "        (data_sets.test, 'test')]:\n",
    "\n",
    "        inception_features_path = 'output/%s_inception_features_new_%s.npz' % (dataset_name, label)\n",
    "        if not os.path.exists(inception_features_path):\n",
    "\n",
    "            print('Inception features do not exist. Generating %s...' % label)\n",
    "            data_set.reset_batch()\n",
    "            \n",
    "            num_examples = data_set.num_examples\n",
    "            assert num_examples % batch_size == 0\n",
    "\n",
    "            inception_features_val = generate_inception_features(\n",
    "                full_model, \n",
    "                data_set.x, \n",
    "                data_set.labels, \n",
    "                batch_size=batch_size)\n",
    "            \n",
    "            np.savez(\n",
    "                inception_features_path, \n",
    "                inception_features_val=inception_features_val,\n",
    "                labels=data_set.labels)\n",
    "            \n",
    "train_f = np.load('output/%s_inception_features_new_train.npz' % dataset_name)\n",
    "inception_X_train = DataSet(train_f['inception_features_val'], train_f['labels'])\n",
    "test_f = np.load('output/%s_inception_features_new_test.npz' % dataset_name)\n",
    "inception_X_test = DataSet(test_f['inception_features_val'], test_f['labels'])\n",
    "\n",
    "validation = None\n",
    "\n",
    "inception_data_sets = base.Datasets(train=inception_X_train, validation=validation, test=inception_X_test)\n",
    "\n",
    "print('*** Top:')\n",
    "with top_graph.as_default():\n",
    "    top_model_name = '%s_inception_onlytop_wd-%s' % (dataset_name, weight_decay)\n",
    "    input_dim = 2048\n",
    "    top_model = BinaryLogisticRegressionWithLBFGS(\n",
    "        input_dim=input_dim,\n",
    "        weight_decay=weight_decay,\n",
    "        max_lbfgs_iter=max_lbfgs_iter,\n",
    "        num_classes=num_classes, \n",
    "        batch_size=batch_size,\n",
    "        data_sets=inception_data_sets,\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        keep_probs=keep_probs,\n",
    "        decay_epochs=decay_epochs,\n",
    "        mini_batch=False,\n",
    "        train_dir='output',\n",
    "        log_dir='log',\n",
    "        model_name=top_model_name)\n",
    "    weights = top_model.retrain_and_get_weights(inception_X_train.x, inception_X_train.labels)\n",
    "    orig_weight_path = 'output/inception_weights_%s.npy' % top_model_name\n",
    "    np.save(orig_weight_path, weights)\n",
    "    \n",
    "with full_graph.as_default():\n",
    "    full_model.load_weights_from_disk(orig_weight_path, do_save=False, do_check=True)\n",
    "\n",
    "### Create poisoned dataset\n",
    "print('Creating poisoned dataset...')\n",
    "\n",
    "step_size = 0.02\n",
    "\n",
    "num_train = len(data_sets.train.labels)\n",
    "num_test = len(data_sets.test.labels)\n",
    "max_num_to_poison = 10\n",
    "\n",
    "### Try attacking each test example individually\n",
    "\n",
    "orig_X_train = np.copy(data_sets.train.x)\n",
    "orig_Y_train = np.copy(data_sets.train.labels)\n",
    "\n",
    "test_indices_to_attack = [3]\n",
    "\n",
    "for test_idx in test_indices_to_attack:\n",
    "\n",
    "    print('****** Attacking test_idx %s ******' % test_idx)\n",
    "    test_description = test_idx\n",
    "\n",
    "    # If this has already been successfully attacked, skip\n",
    "    filenames = [filename for filename in os.listdir('./output') if (\n",
    "        (('%s_attack_testidx-%s_trainidx-' % (full_model.model_name, test_description)) in filename) and        \n",
    "        (filename.endswith('stepsize-%s_proj_final.npz' % step_size)))]\n",
    "        # and (('stepsize-%s_proj_final.npz' % step_size) in filename))] # Check all step sizes        \n",
    "    if len(filenames) > 0:\n",
    "        print('test_idx %s has already been successfully attacked. Skipping...')\n",
    "        continue\n",
    "        \n",
    "    # Use top model to quickly generate inverse HVP\n",
    "    test_inception_single_data = DataSet(np.array([inception_X_test.x[test_idx, :]]), np.array([inception_X_test.labels[test_idx]]))\n",
    "    test_single_data = DataSet(np.array([data_sets.test.x[test_idx, :]]), np.array([data_sets.test.labels[test_idx]]))\n",
    "    with top_graph.as_default():\n",
    "        get_hvp(\n",
    "            top_model,\n",
    "            test_inception_single_data, inception_X_train,\n",
    "            test_description=test_description,\n",
    "            force_refresh=True)\n",
    "    copyfile(\n",
    "        'output/%s-test-%s.npz' % (top_model_name, test_description),\n",
    "        'output/%s-test-%s.npz' % (full_model_name, test_description))\n",
    "        \n",
    "    # Use full model to select indices to poison\n",
    "    with full_graph.as_default():\n",
    "        influence_on_test_loss_val = get_influence_on_test_loss(\n",
    "                    full_model,\n",
    "                    test_single_data, \n",
    "                    np.arange(num_train), data_sets.train,\n",
    "                    test_description,\n",
    "                    force_refresh = False)\n",
    "        # save into file for caching ##### TODO\n",
    "        print(\"finished calculating grad_wrt_input_val\")\n",
    "        all_indices_to_poison = select_examples_to_attack(\n",
    "            full_model, \n",
    "            max_num_to_poison, \n",
    "            influence_on_test_loss_val,\n",
    "            step_size=step_size)\n",
    "\n",
    "    for num_to_poison in [0.1]:\n",
    "        # If we're just attacking one training example, try attacking the first one and also the second one separately\n",
    "        if num_to_poison == 0.1:\n",
    "            indices_to_poison = all_indices_to_poison[0:1]\n",
    "        elif num_to_poison == 1.2:\n",
    "            indices_to_poison = all_indices_to_poison[1:2]\n",
    "        else:\n",
    "            indices_to_poison = all_indices_to_poison[:num_to_poison]\n",
    "        orig_X_train_subset = np.copy(data_sets.train.x[indices_to_poison, :])\n",
    "        orig_X_train_inception_features_subset = np.copy(inception_X_train.x[indices_to_poison, :])\n",
    "\n",
    "        project_fn = get_projection_to_box_around_orig_point(orig_X_train_subset, box_radius_in_pixels=0.5)\n",
    "\n",
    "        attack_success = iterative_attack(top_model, full_model, top_graph, full_graph, project_fn, \n",
    "                                          [test_idx], \n",
    "                                          test_description, \n",
    "                                          data_sets.train, data_sets.test, dataset_name,\n",
    "                                          indices_to_poison=indices_to_poison,\n",
    "                                          num_iter=100,\n",
    "                                          step_size=step_size,\n",
    "                                          save_iter=100,\n",
    "                                          early_stop=0.5)\n",
    "\n",
    "        with full_graph.as_default():\n",
    "            data_sets.train.x[indices_to_poison, :] = orig_X_train_subset\n",
    "            full_model.load_weights_from_disk(orig_weight_path, do_save=False, do_check=False)\n",
    "        with top_graph.as_default():\n",
    "            inception_X_train.x[indices_to_poison, :] = orig_X_train_inception_features_subset\n",
    "            top_model.retrain_and_get_weights(inception_X_train.x, inception_X_train.labels)\n",
    "\n",
    "        if attack_success:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
