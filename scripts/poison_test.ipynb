{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import IPython\n",
    "import copy\n",
    "from shutil import copyfile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "\n",
    "import sys\n",
    "sys.dont_write_bytecode=True\n",
    "\n",
    "PACKAGE_PARENT = '../'\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser('__file__'))))\n",
    "sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
    "\n",
    "from influence.inceptionModel import BinaryInceptionModel\n",
    "from influence.binaryLogisticRegressionWithLBFGS import BinaryLogisticRegressionWithLBFGS\n",
    "import influence.experiments\n",
    "from influence.dataset import DataSet\n",
    "# from influence.dataset_poisoning import iterative_attack, select_examples_to_attack, get_projection_to_box_around_orig_point, generate_inception_features\n",
    "from influence.iter_attack import iterative_attack, select_examples_to_attack, get_projection_to_box_around_orig_point, generate_inception_features\n",
    "from influence.Progress import *\n",
    "\n",
    "from load_animals import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Koda from disk...\n"
     ]
    }
   ],
   "source": [
    "img_side = 299\n",
    "num_channels = 3\n",
    " \n",
    "initial_learning_rate = 0.001 \n",
    "keep_probs = None\n",
    "decay_epochs = [1000, 10000]\n",
    "\n",
    "weight_decay = 0.001\n",
    "\n",
    "num_classes = 2\n",
    "max_lbfgs_iter = 1000\n",
    "\n",
    "num_train_ex_per_class = 200 #900\n",
    "num_test_ex_per_class = 100 #300\n",
    "batch_size = 100\n",
    "\n",
    "dataset_name = 'dogfish_%s_%s' % (num_train_ex_per_class, num_test_ex_per_class)\n",
    "# data_sets = load_animals(\n",
    "#     num_train_ex_per_class=num_train_ex_per_class, \n",
    "#     num_test_ex_per_class=num_test_ex_per_class,\n",
    "#     classes=['dog', 'fish'])\n",
    "# data_sets = load_dogfish_with_orig_and_koda()\n",
    "# X, Y = load_koda()\n",
    "# print(X.shape, Y.shape)\n",
    "data_sets = new_load_dogfish_with_koda()\n",
    "\n",
    "full_graph = tf.Graph()\n",
    "top_graph = tf.Graph()\n",
    "\n",
    "dummy_data_sets = load_dummy(num_train_ex_per_class=num_train_ex_per_class, \n",
    "    num_test_ex_per_class=num_test_ex_per_class,\n",
    "    classes=['dog', 'fish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Full:\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1168: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Total number of parameters: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Top:\n",
      "Total number of parameters: 2048\n",
      "Using normal model\n",
      "LBFGS training took [41] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.01212904\n",
      "Train loss (w/o reg) on all data: 0.003976129\n",
      "Test loss (w/o reg) on all data: 0.048454028\n",
      "Train acc on all data:  1.0\n",
      "Test acc on all data:   0.985\n",
      "Norm of the mean of gradients: 3.6107753e-07\n",
      "Norm of the params: 4.0380473\n",
      "Loaded weights from disk.\n",
      "Train loss (w reg) on all data: [0.01212904]\n",
      "Train loss (w/o reg) on all data: [0.00397613]\n",
      "Test loss (w/o reg) on all data: [0.04845403]\n",
      "Train acc on all data:  [1.]\n",
      "Test acc on all data:   [0.985]\n",
      "Norm of the mean of gradients: 3.6051458e-07\n",
      "Norm of the params: 4.0380473\n",
      "Creating poisoned dataset...\n",
      "****** Attacking test_idx 3 ******\n",
      "Norm of test gradient: 0.03934304\n",
      "Total number of parameters: 2048\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.171727\n",
      "         Iterations: 7\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 14\n",
      "         Hessian evaluations: 39\n",
      "Saved inverse HVP to output/dogfish_200_100_inception_onlytop_wd-0.001-test-3.npz\n",
      "Inverse HVP took 55.0285761356 sec\n",
      "Norm of test gradient: 0.03934304\n",
      "Loaded inverse HVP from output/dogfish_200_100_inception_wd-0.001-test-3.npz\n",
      "Inverse HVP took 0.00112581253052 sec\n",
      "Entering the for loop\n",
      "calculating grad_influence_wrt_input\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool\n\t [[Node: batch_normalization_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op u'batch_normalization_1/keras_learning_phase', defined at:\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tornado/ioloop.py\", line 1009, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tornado/ioloop.py\", line 760, in _run_callback\n    ret = callback()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-5af3e5c45dea>\", line 17, in <module>\n    model_name=full_model_name)\n  File \"/home/ubuntu/influence-release/influence/inceptionModel.py\", line 41, in __init__\n    super(BinaryInceptionModel, self).__init__(**kwargs)\n  File \"/home/ubuntu/influence-release/influence/genericNeuralNet.py\", line 130, in __init__\n    self.logits = self.inference(self.input_placeholder)\n  File \"/home/ubuntu/influence-release/influence/inceptionModel.py\", line 164, in inference\n    self.inception_model = InceptionV3(include_top=False, weights='imagenet', input_tensor=reshaped_input)\n  File \"/home/ubuntu/influence-release/influence/inception_v3.py\", line 169, in InceptionV3\n    x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding='valid')\n  File \"/home/ubuntu/influence-release/influence/inception_v3.py\", line 82, in conv2d_bn\n    x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/engine/topology.py\", line 585, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/layers/normalization.py\", line 190, in call\n    training=training)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 2559, in in_train_phase\n    training = learning_phase()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 112, in learning_phase\n    name='keras_learning_phase')\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1746, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3051, in _placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool\n\t [[Node: batch_normalization_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5af3e5c45dea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                     \u001b[0mtest_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                     force_refresh = False)\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finished calculating grad_wrt_input_val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         all_indices_to_poison = select_examples_to_attack(\n",
      "\u001b[0;32m/home/ubuntu/influence-release/influence/Progress.py\u001b[0m in \u001b[0;36mget_grad_of_influence_wrt_input\u001b[0;34m(model, test_data, train_idx, train_data, test_description, force_refresh)\u001b[0m\n\u001b[1;32m    224\u001b[0m       current_grad_influence_wrt_input_val = model.grad_influence_wrt_input(inverse_hvp, \n\u001b[1;32m    225\u001b[0m                                                       \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_to_remove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                                                       train_data.labels[idx_to_remove].reshape(-1))\n\u001b[0m\u001b[1;32m    227\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgrad_influence_wrt_input_val\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m           \u001b[0mgrad_influence_wrt_input_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_grad_influence_wrt_input_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/influence-release/influence/genericNeuralNet.py\u001b[0m in \u001b[0;36mgrad_influence_wrt_input\u001b[0;34m(self, inverse_hvp, xTr, yTr)\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"calculating grad_influence_wrt_input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_feed_dict_with_v_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse_hvp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_influence_wrt_input_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrad_loss_no_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool\n\t [[Node: batch_normalization_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op u'batch_normalization_1/keras_learning_phase', defined at:\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tornado/ioloop.py\", line 1009, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tornado/ioloop.py\", line 760, in _run_callback\n    ret = callback()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-5af3e5c45dea>\", line 17, in <module>\n    model_name=full_model_name)\n  File \"/home/ubuntu/influence-release/influence/inceptionModel.py\", line 41, in __init__\n    super(BinaryInceptionModel, self).__init__(**kwargs)\n  File \"/home/ubuntu/influence-release/influence/genericNeuralNet.py\", line 130, in __init__\n    self.logits = self.inference(self.input_placeholder)\n  File \"/home/ubuntu/influence-release/influence/inceptionModel.py\", line 164, in inference\n    self.inception_model = InceptionV3(include_top=False, weights='imagenet', input_tensor=reshaped_input)\n  File \"/home/ubuntu/influence-release/influence/inception_v3.py\", line 169, in InceptionV3\n    x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding='valid')\n  File \"/home/ubuntu/influence-release/influence/inception_v3.py\", line 82, in conv2d_bn\n    x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/engine/topology.py\", line 585, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/layers/normalization.py\", line 190, in call\n    training=training)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 2559, in in_train_phase\n    training = learning_phase()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 112, in learning_phase\n    name='keras_learning_phase')\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1746, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3051, in _placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool\n\t [[Node: batch_normalization_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "print('*** Full:')\n",
    "with full_graph.as_default():\n",
    "    full_model_name = '%s_inception_wd-%s' % (dataset_name, weight_decay)\n",
    "    full_model = BinaryInceptionModel(\n",
    "        img_side=img_side,\n",
    "        num_channels=num_channels,\n",
    "        weight_decay=weight_decay,\n",
    "        num_classes=num_classes, \n",
    "        batch_size=batch_size,\n",
    "        data_sets=data_sets,\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        keep_probs=keep_probs,\n",
    "        decay_epochs=decay_epochs,\n",
    "        mini_batch=True,\n",
    "        train_dir='output',\n",
    "        log_dir='log',\n",
    "        model_name=full_model_name)\n",
    "    \n",
    "    for data_set, label in [\n",
    "        (data_sets.train, 'train'),\n",
    "        (data_sets.test, 'test')]:\n",
    "\n",
    "        inception_features_path = 'output/%s_inception_features_new_%s.npz' % (dataset_name, label)\n",
    "        if not os.path.exists(inception_features_path):\n",
    "\n",
    "            print('Inception features do not exist. Generating %s...' % label)\n",
    "            data_set.reset_batch()\n",
    "            \n",
    "            num_examples = data_set.num_examples\n",
    "            assert num_examples % batch_size == 0\n",
    "\n",
    "            inception_features_val = generate_inception_features(\n",
    "                full_model, \n",
    "                data_set.x, \n",
    "                data_set.labels, \n",
    "                batch_size=batch_size)\n",
    "            \n",
    "            np.savez(\n",
    "                inception_features_path, \n",
    "                inception_features_val=inception_features_val,\n",
    "                labels=data_set.labels)\n",
    "            \n",
    "train_f = np.load('output/%s_inception_features_new_train.npz' % dataset_name)\n",
    "inception_X_train = DataSet(train_f['inception_features_val'], train_f['labels'])\n",
    "test_f = np.load('output/%s_inception_features_new_test.npz' % dataset_name)\n",
    "inception_X_test = DataSet(test_f['inception_features_val'], test_f['labels'])\n",
    "\n",
    "validation = None\n",
    "\n",
    "inception_data_sets = base.Datasets(train=inception_X_train, validation=validation, test=inception_X_test)\n",
    "\n",
    "print('*** Top:')\n",
    "with top_graph.as_default():\n",
    "    top_model_name = '%s_inception_onlytop_wd-%s' % (dataset_name, weight_decay)\n",
    "    input_dim = 2048\n",
    "    top_model = BinaryLogisticRegressionWithLBFGS(\n",
    "        input_dim=input_dim,\n",
    "        weight_decay=weight_decay,\n",
    "        max_lbfgs_iter=max_lbfgs_iter,\n",
    "        num_classes=num_classes, \n",
    "        batch_size=batch_size,\n",
    "        data_sets=inception_data_sets,\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        keep_probs=keep_probs,\n",
    "        decay_epochs=decay_epochs,\n",
    "        mini_batch=False,\n",
    "        train_dir='output',\n",
    "        log_dir='log',\n",
    "        model_name=top_model_name)\n",
    "    weights = top_model.retrain_and_get_weights(inception_X_train.x, inception_X_train.labels)\n",
    "    orig_weight_path = 'output/inception_weights_%s.npy' % top_model_name\n",
    "    np.save(orig_weight_path, weights)\n",
    "    \n",
    "with full_graph.as_default():\n",
    "    full_model.load_weights_from_disk(orig_weight_path, do_save=False, do_check=True)\n",
    "\n",
    "### Create poisoned dataset\n",
    "print('Creating poisoned dataset...')\n",
    "\n",
    "step_size = 0.02\n",
    "\n",
    "num_train = len(data_sets.train.labels)\n",
    "num_test = len(data_sets.test.labels)\n",
    "max_num_to_poison = 10\n",
    "\n",
    "### Try attacking each test example individually\n",
    "\n",
    "orig_X_train = np.copy(data_sets.train.x)\n",
    "orig_Y_train = np.copy(data_sets.train.labels)\n",
    "\n",
    "test_indices_to_attack = [3]\n",
    "\n",
    "for test_idx in test_indices_to_attack:\n",
    "\n",
    "    print('****** Attacking test_idx %s ******' % test_idx)\n",
    "    test_description = test_idx\n",
    "\n",
    "    # If this has already been successfully attacked, skip\n",
    "    filenames = [filename for filename in os.listdir('./output') if (\n",
    "        (('%s_attack_testidx-%s_trainidx-' % (full_model.model_name, test_description)) in filename) and        \n",
    "        (filename.endswith('stepsize-%s_proj_final.npz' % step_size)))]\n",
    "        # and (('stepsize-%s_proj_final.npz' % step_size) in filename))] # Check all step sizes        \n",
    "    if len(filenames) > 0:\n",
    "        print('test_idx %s has already been successfully attacked. Skipping...')\n",
    "        continue\n",
    "        \n",
    "    # Use top model to quickly generate inverse HVP\n",
    "    test_inception_single_data = DataSet(np.array([inception_X_test.x[test_idx, :]]), np.array([inception_X_test.labels[test_idx]]))\n",
    "    test_single_data = DataSet(np.array([data_sets.test.x[test_idx, :]]), np.array([data_sets.test.labels[test_idx]]))\n",
    "    with top_graph.as_default():\n",
    "        get_hvp(\n",
    "            top_model,\n",
    "            test_inception_single_data, inception_X_train,\n",
    "            test_description=test_description,\n",
    "            force_refresh=True)\n",
    "    copyfile(\n",
    "        'output/%s-test-%s.npz' % (top_model_name, test_description),\n",
    "        'output/%s-test-%s.npz' % (full_model_name, test_description))\n",
    "        \n",
    "    # Use full model to select indices to poison\n",
    "    with full_graph.as_default():\n",
    "        grad_influence_wrt_input_val = get_grad_of_influence_wrt_input(\n",
    "                    full_model,\n",
    "                    test_single_data, \n",
    "                    np.arange(num_train), data_sets.train,\n",
    "                    test_description,\n",
    "                    force_refresh = False)\n",
    "        print(\"finished calculating grad_wrt_input_val\")\n",
    "        all_indices_to_poison = select_examples_to_attack(\n",
    "            full_model, \n",
    "            max_num_to_poison, \n",
    "            grad_influence_wrt_input_val,\n",
    "            step_size=step_size)\n",
    "\n",
    "    for num_to_poison in [0.1]:\n",
    "        # If we're just attacking one training example, try attacking the first one and also the second one separately\n",
    "        if num_to_poison == 0.1:\n",
    "            indices_to_poison = all_indices_to_poison[0:1]\n",
    "        elif num_to_poison == 1.2:\n",
    "            indices_to_poison = all_indices_to_poison[1:2]\n",
    "        else:\n",
    "            indices_to_poison = all_indices_to_poison[:num_to_poison]\n",
    "        orig_X_train_subset = np.copy(data_sets.train.x[indices_to_poison, :])\n",
    "        orig_X_train_inception_features_subset = np.copy(inception_X_train.x[indices_to_poison, :])\n",
    "\n",
    "        project_fn = get_projection_to_box_around_orig_point(orig_X_train_subset, box_radius_in_pixels=0.5)\n",
    "\n",
    "        attack_success = iterative_attack(top_model, full_model, top_graph, full_graph, project_fn, \n",
    "                                          [test_idx], \n",
    "                                          test_description, \n",
    "                                          data_sets.train, data_sets.test,\n",
    "                                          indices_to_poison=indices_to_poison,\n",
    "                                          num_iter=100,\n",
    "                                          step_size=step_size,\n",
    "                                          save_iter=100,\n",
    "                                          early_stop=0.5)\n",
    "\n",
    "        with full_graph.as_default():\n",
    "            data_sets.train.x[indices_to_poison, :] = orig_X_train_subset\n",
    "            full_model.load_weights_from_disk(orig_weight_path, do_save=False, do_check=False)\n",
    "        with top_graph.as_default():\n",
    "            inception_X_train.x[indices_to_poison, :] = orig_X_train_inception_features_subset\n",
    "            top_model.retrain_and_get_weights(inception_X_train.x, inception_X_train.labels)\n",
    "\n",
    "        if attack_success:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_influence_wrt_input_val = get_grad_of_influence_wrt_input(\n",
    "                    full_model,\n",
    "                    test_single_data, \n",
    "                    np.arange(num_train), data_sets.train,\n",
    "                    test_description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p27]",
   "language": "python",
   "name": "conda-env-tensorflow_p27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
